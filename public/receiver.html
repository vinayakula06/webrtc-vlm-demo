<!-- public/receiver.html -->
<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Real-time WebRTC VLM Multi-Object Detection</title>
  <style>
    body {
      font-family: 'Inter', system-ui, sans-serif;
      background: linear-gradient(120deg, #f8fafc 0%, #e0e7ef 100%);
      margin: 0;
      padding: 0;
      color: #222;
    }
    .container {
      max-width: 1200px;
      margin: 32px auto;
      background: #fff;
      border-radius: 18px;
      box-shadow: 0 8px 32px rgba(60,80,120,0.10);
      padding: 32px 24px 24px 24px;
    }
    h3 {
      color: #1a237e;
      margin-bottom: 28px;
      font-size: 2.2rem;
      font-weight: 700;
      letter-spacing: -1px;
    }
    #stage {
      position: relative;
      width: 100%;
      max-width: 720px;
      margin-bottom: 24px;
      box-shadow: 0 6px 24px rgba(60,80,120,0.13);
      border-radius: 16px;
      overflow: hidden;
      background: #111;
      display: flex;
      justify-content: center;
      align-items: center;
    }
    video {
      width: 100%;
      height: auto;
      background: #000;
      display: block;
      border-radius: 16px;
      object-fit: contain;
      max-height: 80vh;
    }
    video::-webkit-media-controls {
      display: none !important;
    }
    canvas {
      position: absolute;
      left: 50%;
      top: 50%;
      transform: translate(-50%, -50%);
      pointer-events: none;
      border-radius: 16px;
    }
    #qrSection {
      margin-top: 24px;
      padding: 24px;
      background: #f6f8fa;
      border-radius: 14px;
      box-shadow: 0 2px 8px rgba(60,80,120,0.07);
    }
    #qrCode {
      max-width: 180px;
      margin: 12px 0;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(60,80,120,0.10);
    }
    .main-content {
      display: flex;
      gap: 32px;
      flex-wrap: wrap;
    }
    .video-section {
      flex: 1;
      min-width: 0;
      width: 100%;
    }
    .info-section {
      flex: 0 0 320px;
    }
    @media (max-width: 900px) {
      .main-content { flex-direction: column; gap: 16px; }
      .video-section, .info-section { min-width: 0; width: 100%; }
      #stage { width: 100%; }
    }
  </style>
</head>
<body>
  <div class="container">
  <h3>üéØ Real-time WebRTC VLM<br>Multi-Object Detection</h3>
    <div class="main-content">
      <div class="video-section">
        <div id="stage">
          <video id="remoteVideo" autoplay muted playsinline></video>
          <canvas id="overlay"></canvas>
        </div>
        <div id="status" style="margin-top: 12px; padding: 12px; background: rgba(26,35,126,0.1); border-radius: 8px; text-align: center;">
          üì± Waiting for phone connection...
        </div>
      </div>
      <div class="info-section">
        <div id="qrSection">
          <h4>üì± Connect Your Phone</h4>
          <p>Scan this QR code with your phone to start streaming:</p>
          <img id="qrCode" alt="QR Code" style="display:none;" />
          <div id="qrUrl"></div>
          <p><small>Or visit the URL directly on your phone's browser</small></p>
        </div>
        
        <!-- Live Metrics Panel -->
        <div id="metricsSection" style="margin-top: 20px; padding: 16px; background: rgba(26,35,126,0.05); border-radius: 12px; border: 1px solid rgba(26,35,126,0.1);">
          <h4 style="margin: 0 0 12px 0; color: #1a237e;">üìä Live Metrics</h4>
          
          <!-- AI Model Info -->
          <div style="margin-bottom: 12px; padding: 6px; background: #f8f9fa; border-radius: 6px; font-size: 0.8em; text-align: center;">
            <div style="color: #666;">AI Model</div>
            <div id="modelInfo" style="font-weight: bold; color: #1a237e;">Loading...</div>
          </div>
          
          <div style="margin-bottom: 12px;">
            <div style="background: #fff; padding: 8px; border-radius: 6px; text-align: center; border: 1px solid #e0e7ef;">
              <div style="font-size: 0.8em; color: #666;">Objects</div>
              <div id="objectCount" style="font-weight: bold; color: #28a745;">0</div>
            </div>
          </div>
          
          <div style="margin-bottom: 12px;">
            <button id="startBenchBtn" onclick="startBenchmark()" style="width: 100%; padding: 8px; background: #1a237e; color: white; border: none; border-radius: 6px; cursor: pointer; font-size: 0.9em;">
              üìà Start 30s Benchmark
            </button>
          </div>
          
          <div style="margin-bottom: 12px;">
            <button id="downloadMetricsBtn" onclick="downloadMetrics()" style="width: 100%; padding: 8px; background: #28a745; color: white; border: none; border-radius: 6px; cursor: pointer; font-size: 0.9em;" disabled>
              üíæ Download metrics.json
            </button>
          </div>
          
          <div id="benchStatus" style="display: none; padding: 8px; background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 6px; font-size: 0.9em; text-align: center;">
            Benchmark running...
          </div>
        </div>
      </div>
    </div>
  </div>

  <script src="/socket.io/socket.io.js"></script>
  <!-- Multiple AI Model Support: ONNX, TensorFlow Lite, and COCO-SSD fallback -->
  <!-- ONNX Runtime Web for better models -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.3/dist/ort.min.js"></script>
  <!-- TensorFlow.js + COCO-SSD (fallback model) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>

  <script>
  const url = new URL(location.href);
  const room = url.searchParams.get('room') || 'room1';
  const MODE = (url.searchParams.get('mode') || 'wasm'); // used only for UI hints

  // ========== ENHANCED MODEL MANAGER FOR MULTIPLE AI MODELS ==========
  
  class ModelManager {
    constructor() {
      this.currentModel = null;
      this.modelType = null;
      this.isLoaded = false;
      this.currentConfig = null;
      
      // Performance tracking
      this.performanceMetrics = {
        totalInferences: 0,
        averageTime: 0,
        lastInferenceTime: 0,
        successRate: 0,
        failures: 0
      };
      
      // Model configurations - prioritized by performance
      this.modelConfigs = [
        {
          name: 'YOLOv8n-Local',
          type: 'onnx',
          url: '/models/yolov8n.onnx', // Try local first
          inputSize: [640, 640],
          classes: 80,
          description: 'YOLOv8 Nano - Fast and accurate (Local)'
        },
        {
          name: 'YOLOv8n-CDN',
          type: 'onnx',
          url: 'https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.onnx',
          inputSize: [640, 640],
          classes: 80,
          description: 'YOLOv8 Nano - Fast and accurate (CDN)'
        },
        {
          name: 'MobileNet-SSD-Local',
          type: 'onnx', 
          url: '/models/ssd_mobilenet_v1_10.onnx',
          inputSize: [300, 300],
          classes: 91,
          description: 'MobileNet SSD - Optimized for mobile (Local)'
        },
        {
          name: 'MobileNet-SSD-CDN',
          type: 'onnx', 
          url: 'https://github.com/onnx/models/raw/main/vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_10.onnx',
          inputSize: [300, 300],
          classes: 91,
          description: 'MobileNet SSD - Optimized for mobile (CDN)'
        },
        {
          name: 'COCO-SSD-TensorFlow',
          type: 'tensorflow',
          url: 'builtin',
          inputSize: [300, 300],
          classes: 80,
          description: 'COCO-SSD - Reliable fallback'
        }
      ];
      
      // COCO class names for consistent labeling
      this.cocoClasses = [
        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',
        'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
        'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',
        'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
        'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
        'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
        'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
        'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
        'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
        'toothbrush'
      ];
    }

    async loadBestAvailableModel() {
      console.log('üöÄ Loading best available AI model...');
      
      // First, try to discover local models
      await this.discoverLocalModels();
      
      for (const config of this.modelConfigs) {
        try {
          console.log(`üîÑ Trying to load ${config.name}...`);
          
          if (config.type === 'onnx') {
            const success = await this.loadONNXModel(config);
            if (success) {
              this.modelType = 'onnx';
              this.currentConfig = config;
              this.isLoaded = true;
              console.log(`‚úÖ Successfully loaded ${config.name}`);
              return true;
            }
          } else if (config.type === 'tensorflow') {
            const success = await this.loadTensorFlowModel(config);
            if (success) {
              this.modelType = 'tensorflow';
              this.currentConfig = config;
              this.isLoaded = true;
              console.log(`‚úÖ Successfully loaded ${config.name}`);
              return true;
            }
          }
        } catch (error) {
          console.warn(`‚ö†Ô∏è Failed to load ${config.name}:`, error.message);
          continue; // Try next model
        }
      }
      
      console.error('‚ùå All models failed to load');
      return false;
    }

    async discoverLocalModels() {
      try {
        const response = await fetch('/api/models');
        const data = await response.json();
        
        if (data.models && data.models.length > 0) {
          console.log('üìÅ Found local models:', data.models);
          
          // Add discovered models to the beginning of the config list
          const discoveredConfigs = data.models.map(model => {
            if (model.name.toLowerCase().includes('yolo') && model.type === 'onnx') {
              return {
                name: `${model.name} (Local)`,
                type: 'onnx',
                url: model.path,
                inputSize: [640, 640],
                classes: 80,
                description: `Local YOLO model - ${(model.size / 1024 / 1024).toFixed(1)}MB`
              };
            } else if (model.name.toLowerCase().includes('mobilenet') && model.type === 'onnx') {
              return {
                name: `${model.name} (Local)`,
                type: 'onnx',
                url: model.path,
                inputSize: [300, 300],
                classes: 91,
                description: `Local MobileNet model - ${(model.size / 1024 / 1024).toFixed(1)}MB`
              };
            } else if (model.type === 'onnx') {
              return {
                name: `${model.name} (Local)`,
                type: 'onnx',
                url: model.path,
                inputSize: [640, 640], // Default size
                classes: 80,
                description: `Local ONNX model - ${(model.size / 1024 / 1024).toFixed(1)}MB`
              };
            }
            return null;
          }).filter(Boolean);
          
          // Prepend discovered models (highest priority)
          this.modelConfigs = [...discoveredConfigs, ...this.modelConfigs];
        }
      } catch (error) {
        console.log('üìÅ No local models API available:', error.message);
      }
    }

    async loadONNXModel(config) {
      try {
        // Check if ONNX Runtime is available
        if (typeof ort === 'undefined') {
          throw new Error('ONNX Runtime not available');
        }

        // Create ONNX session
        this.currentModel = await ort.InferenceSession.create(config.url);
        console.log(`üìä ONNX Model loaded: ${config.name}`);
        console.log('Input names:', this.currentModel.inputNames);
        console.log('Output names:', this.currentModel.outputNames);
        return true;
      } catch (error) {
        console.warn(`ONNX model load failed: ${error.message}`);
        return false;
      }
    }

    async loadTensorFlowModel(config) {
      try {
        // Use built-in COCO-SSD model
        this.currentModel = await cocoSsd.load();
        console.log(`üìä TensorFlow Model loaded: ${config.name}`);
        return true;
      } catch (error) {
        console.warn(`TensorFlow model load failed: ${error.message}`);
        return false;
      }
    }

    async detect(imageData) {
      if (!this.isLoaded || !this.currentModel) {
        console.warn('‚ö†Ô∏è Model not loaded, attempting to load...');
        const loaded = await this.loadBestAvailableModel();
        if (!loaded) {
          throw new Error('No AI model available for detection');
        }
      }

      const startTime = performance.now();
      let detections = [];

      try {
        if (this.modelType === 'onnx') {
          detections = await this.detectONNX(imageData);
        } else if (this.modelType === 'tensorflow') {
          detections = await this.detectTensorFlow(imageData);
        }

        const endTime = performance.now();
        const inferenceTime = endTime - startTime;
        
        // Update performance metrics
        this.performanceMetrics.totalInferences++;
        this.performanceMetrics.lastInferenceTime = inferenceTime;
        this.performanceMetrics.averageTime = 
          (this.performanceMetrics.averageTime * (this.performanceMetrics.totalInferences - 1) + inferenceTime) / 
          this.performanceMetrics.totalInferences;

        console.log(`üéØ Detection completed in ${inferenceTime.toFixed(1)}ms, found ${detections.length} objects with ${this.currentConfig?.name}`);
        
        return detections;
      } catch (error) {
        console.error(`‚ùå Detection failed with ${this.currentConfig?.name}:`, error);
        this.performanceMetrics.failures++;
        
        // Attempt recovery with fallback model
        if (this.modelType !== 'tensorflow' || this.currentConfig?.name !== 'COCO-SSD-TensorFlow') {
          console.log('üîÑ Attempting recovery with fallback model...');
          return await this.fallbackToBasicModel(imageData);
        }
        
        throw error;
      }
    }

    async fallbackToBasicModel(imageData) {
      try {
        console.log('üö® Using emergency fallback to COCO-SSD...');
        const success = await this.loadTensorFlowModel({
          name: 'COCO-SSD-TensorFlow',
          type: 'tensorflow',
          url: 'builtin',
          classes: 80,
          description: 'Emergency fallback model'
        });
        
        if (success) {
          this.modelType = 'tensorflow';
          this.currentConfig = {
            name: 'COCO-SSD-TensorFlow',
            type: 'tensorflow',
            url: 'builtin',
            classes: 80,
            description: 'Emergency fallback model'
          };
          this.isLoaded = true;
          return await this.detectTensorFlow(imageData);
        }
      } catch (fallbackError) {
        console.error('‚ùå Even fallback model failed:', fallbackError);
        throw new Error('All AI models failed - system unable to perform detection');
      }
    }

    async detectONNX(canvas) {
      try {
        const config = this.currentConfig;
        const [inputHeight, inputWidth] = config.inputSize;
        
        // Prepare input tensor
        const ctx = canvas.getContext('2d');
        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
        
        // Resize and normalize image data for ONNX model
        const resizedCanvas = document.createElement('canvas');
        resizedCanvas.width = inputWidth;
        resizedCanvas.height = inputHeight;
        const resizedCtx = resizedCanvas.getContext('2d');
        resizedCtx.drawImage(canvas, 0, 0, inputWidth, inputHeight);
        
        const resizedImageData = resizedCtx.getImageData(0, 0, inputWidth, inputHeight);
        const float32Data = new Float32Array(3 * inputHeight * inputWidth);
        
        // Convert to normalized RGB format
        for (let i = 0; i < resizedImageData.data.length; i += 4) {
          const pixelIndex = i / 4;
          float32Data[pixelIndex] = resizedImageData.data[i] / 255.0; // R
          float32Data[pixelIndex + inputHeight * inputWidth] = resizedImageData.data[i + 1] / 255.0; // G  
          float32Data[pixelIndex + 2 * inputHeight * inputWidth] = resizedImageData.data[i + 2] / 255.0; // B
        }

        const inputTensor = new ort.Tensor('float32', float32Data, [1, 3, inputHeight, inputWidth]);
        
        // Run inference
        const results = await this.currentModel.run({ images: inputTensor });
        
        // Parse results (format depends on model - this is a general approach)
        return this.parseONNXResults(results, canvas.width, canvas.height);
        
      } catch (error) {
        console.error('ONNX detection error:', error);
        return []; // Return empty array on error
      }
    }

    parseONNXResults(results, originalWidth, originalHeight) {
      const detections = [];
      
      try {
        // Handle YOLOv8 format: outputs[0] = [1, 84, 8400] where 84 = 4 (bbox) + 80 (classes)
        // Handle SSD format: multiple outputs [boxes, scores, classes]
        
        const outputNames = Object.keys(results);
        console.log('ONNX output names:', outputNames);
        console.log('ONNX output shapes:', outputNames.map(name => 
          `${name}: [${results[name].dims.join(', ')}]`));
        
        if (outputNames.length === 1) {
          // YOLOv8 format - single output with shape [1, 84, 8400]
          const output = results[outputNames[0]];
          if (output.dims.length === 3 && output.dims[1] === 84) {
            return this.parseYOLOv8Results(output, originalWidth, originalHeight);
          }
        } else if (outputNames.length >= 3) {
          // SSD format - multiple outputs
          return this.parseSSDResults(results, originalWidth, originalHeight);
        }
        
        console.log('üìä Unknown ONNX model format - using fallback');
        
      } catch (error) {
        console.error('Error parsing ONNX results:', error);
      }
      
      return detections;
    }

    parseYOLOv8Results(output, originalWidth, originalHeight) {
      const detections = [];
      const data = output.data;
      const [batch, channels, numDetections] = output.dims;
      
      // YOLOv8 output format: [x_center, y_center, width, height, conf_class0, conf_class1, ...]
      for (let i = 0; i < numDetections; i++) {
        const offset = i * channels;
        
        // Extract bbox coordinates (normalized 0-1)
        const xCenter = data[offset];
        const yCenter = data[offset + 1]; 
        const width = data[offset + 2];
        const height = data[offset + 3];
        
        // Find best class and confidence
        let maxConf = 0;
        let bestClass = 0;
        for (let c = 4; c < channels; c++) {
          const conf = data[offset + c];
          if (conf > maxConf) {
            maxConf = conf;
            bestClass = c - 4; // Subtract 4 to get class index
          }
        }
        
        // Filter by confidence threshold
        if (maxConf > 0.25) {
          const xmin = Math.max(0, (xCenter - width / 2) / this.currentConfig.inputSize[1]);
          const ymin = Math.max(0, (yCenter - height / 2) / this.currentConfig.inputSize[0]);
          const xmax = Math.min(1, (xCenter + width / 2) / this.currentConfig.inputSize[1]);
          const ymax = Math.min(1, (yCenter + height / 2) / this.currentConfig.inputSize[0]);
          
          detections.push({
            label: this.cocoClasses[bestClass] || `class_${bestClass}`,
            score: maxConf,
            xmin,
            ymin,
            xmax,
            ymax
          });
        }
      }
      
      console.log(`üìä YOLOv8 parsed ${detections.length} detections`);
      return detections;
    }

    parseSSDResults(results, originalWidth, originalHeight) {
      const detections = [];
      
      try {
        // Look for common SSD output names
        const boxesKey = Object.keys(results).find(key => 
          key.toLowerCase().includes('box') || key.toLowerCase().includes('detection'));
        const scoresKey = Object.keys(results).find(key => 
          key.toLowerCase().includes('score') || key.toLowerCase().includes('conf'));
        const classesKey = Object.keys(results).find(key => 
          key.toLowerCase().includes('class') || key.toLowerCase().includes('label'));
        
        if (boxesKey && scoresKey && classesKey) {
          const boxes = results[boxesKey].data;
          const scores = results[scoresKey].data;
          const classes = results[classesKey].data;
          
          const numDetections = Math.min(boxes.length / 4, scores.length, classes.length);
          
          for (let i = 0; i < numDetections; i++) {
            const score = scores[i];
            if (score > 0.3) {
              const classIndex = Math.floor(classes[i]);
              detections.push({
                label: this.cocoClasses[classIndex] || `class_${classIndex}`,
                score: score,
                xmin: boxes[i * 4],
                ymin: boxes[i * 4 + 1],
                xmax: boxes[i * 4 + 2],
                ymax: boxes[i * 4 + 3]
              });
            }
          }
        }
        
        console.log(`üìä SSD parsed ${detections.length} detections`);
      } catch (error) {
        console.error('Error parsing SSD results:', error);
      }
      
      return detections;
    }

    async detectTensorFlow(canvas) {
      try {
        const predictions = await this.currentModel.detect(canvas);
        
        // Convert to normalized format
        return predictions.map(p => ({
          label: p.class,
          score: p.score,
          xmin: p.bbox[0] / canvas.width,
          ymin: p.bbox[1] / canvas.height,
          xmax: (p.bbox[0] + p.bbox[2]) / canvas.width,
          ymax: (p.bbox[1] + p.bbox[3]) / canvas.height
        }));
      } catch (error) {
        console.error('TensorFlow detection error:', error);
        return [];
      }
    }

    getModelInfo() {
      if (!this.isLoaded) return 'No model loaded';
      return `${this.currentConfig.name} (${this.modelType})`;
    }

    getPerformanceInfo() {
      const successRate = this.performanceMetrics.totalInferences > 0 ? 
        ((this.performanceMetrics.totalInferences - this.performanceMetrics.failures) / this.performanceMetrics.totalInferences * 100).toFixed(1) : 0;
        
      return {
        modelName: this.currentConfig?.name || 'None',
        modelType: this.modelType,
        totalInferences: this.performanceMetrics.totalInferences,
        averageTime: this.performanceMetrics.averageTime.toFixed(1) + 'ms',
        lastTime: this.performanceMetrics.lastInferenceTime.toFixed(1) + 'ms',
        successRate: successRate + '%',
        failures: this.performanceMetrics.failures,
        isLoaded: this.isLoaded
      };
    }
  }

  // Initialize model manager
  const modelManager = new ModelManager();

  // ========== END ENHANCED MODEL MANAGER ==========

  const socket = io();
  // Enhanced WebRTC configuration for better compatibility
  let pc = new RTCPeerConnection({
    // Add STUN servers for better connectivity
    iceServers: [
      { urls: 'stun:stun.l.google.com:19302' },
      { urls: 'stun:stun1.l.google.com:19302' }
    ],
    iceCandidatePoolSize: 10,
    iceTransportPolicy: 'all',
    bundlePolicy: 'max-bundle',
    rtcpMuxPolicy: 'require',
    sdpSemantics: 'unified-plan'
  });
  const video = document.getElementById('remoteVideo');
  const overlay = document.getElementById('overlay');
  const ctx = overlay.getContext('2d');
  let lastMeta = null;
  let detectionsHistory = [];
  let e2eLatencies = [];
  let detectionCount = 0;
  let frameCount = 0;
  let lastStatsUpdate = Date.now();
  let latestDetections = [];
  let senderId = null; // Store sender ID for coordination
  let pendingIceCandidates = []; // Store ICE candidates until senderId is set
  let connectionAttempts = 0; // Track connection attempts
  const MAX_CONNECTION_ATTEMPTS = 3;

  // Function to reset peer connection
  function resetPeerConnection() {
    console.log('üîÑ Resetting peer connection...');
    
    // Close existing connection
    if (pc) {
      pc.close();
    }
    
    // Create new peer connection
    pc = new RTCPeerConnection({
      iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' }
      ],
      iceCandidatePoolSize: 10,
      iceTransportPolicy: 'all',
      bundlePolicy: 'max-bundle',
      rtcpMuxPolicy: 'require',
      sdpSemantics: 'unified-plan'
    });
    
    // Reset state
    pendingIceCandidates = [];
    
    // Re-attach event handlers
    setupPeerConnectionHandlers();
    
    console.log('‚úÖ Peer connection reset complete');
  }

  // Metrics collection variables
  let metricsData = {
    latencies: [],
    fps: [],
    uplink: [],
    downlink: [],
    startTime: null,
    endTime: null,
    totalFrames: 0,
    totalObjects: 0
  };
  let benchmarkActive = false;
  let benchmarkStartTime = null;
  let statsInterval = null;
  let lastBytesReceived = 0;
  let lastBytesSent = 0;
  let lastStatsTime = Date.now();

  window.metricsReady = false;

  // Load QR code on page load
  loadQRCode();

  async function loadQRCode() {
    try {
      const response = await fetch(`/qr/${room}`);
      const data = await response.json();
      if (data.qrCode) {
        document.getElementById('qrCode').src = data.qrCode;
        document.getElementById('qrCode').style.display = 'block';
        
        const protocolBadge = data.useHTTPS ? 'üîí HTTPS' : '‚ö†Ô∏è HTTP';
        const cameraNote = data.useHTTPS ? 
          'Camera access enabled' : 
          'Camera may not work on mobile (needs HTTPS)';
          
        document.getElementById('qrUrl').innerHTML = `
          <strong>${protocolBadge} Network URL:</strong><br>
          <a href="${data.senderUrl}" target="_blank">${data.senderUrl}</a><br>
          <small style="color:#666;">IP: ${data.networkIP || 'N/A'} | Port: ${data.port || '3000'}</small><br>
          <small style="color:${data.useHTTPS ? '#28a745' : '#dc3545'};">${cameraNote}</small>
        `;
      }
    } catch (error) {
      console.error('Failed to load QR code:', error);
      const fallbackUrl = `${location.origin}/sender.html?room=${encodeURIComponent(room)}`;
      document.getElementById('qrUrl').innerHTML = `
        <strong>Fallback URL:</strong><br>
        <a href="${fallbackUrl}" target="_blank">${fallbackUrl}</a><br>
        <small style="color:#f44;">‚ö†Ô∏è Network IP detection failed - using localhost</small>
      `;
    }
  }

  function updateStats() {
    document.getElementById('objectCount').textContent = detectionCount;
    document.getElementById('frameCount').textContent = frameCount;
    
    // Calculate FPS
    const now = Date.now();
    const timeDiff = (now - lastStatsUpdate) / 1000;
    if (timeDiff > 0) {
      const fps = Math.round(frameCount / timeDiff);
      document.getElementById('fps').textContent = fps;
    }
    
    // Show latest latency
    if (e2eLatencies.length > 0) {
      const latestLatency = e2eLatencies[e2eLatencies.length - 1];
      document.getElementById('latency').textContent = `${latestLatency}ms`;
    }
    
    // Update model performance information
    const perfInfo = modelManager.getPerformanceInfo();
    const modelElement = document.getElementById('modelInfo');
    if (modelElement) {
      modelElement.innerHTML = `
        <strong>${perfInfo.modelName}</strong><br>
        <small>Avg: ${perfInfo.averageTime} | Last: ${perfInfo.lastTime} | Success: ${perfInfo.successRate}</small>
      `;
    }
    
    // Update detected objects list
    updateDetectedObjectsList();
  }

  function updateDetectedObjectsList() {
    const objectsList = document.getElementById('objectsList');
    if (latestDetections.length === 0) {
      objectsList.innerHTML = '<em>No objects detected yet...</em>';
      return;
    }
    
    // Group objects by type and count them
    const objectCounts = {};
    latestDetections.forEach(d => {
      const key = d.label;
      if (!objectCounts[key]) {
        objectCounts[key] = { count: 0, maxScore: 0 };
      }
      objectCounts[key].count++;
      objectCounts[key].maxScore = Math.max(objectCounts[key].maxScore, d.score);
    });
    
    // Create list items
    const items = Object.entries(objectCounts)
      .sort((a, b) => b[1].maxScore - a[1].maxScore) // Sort by confidence
      .map(([name, data]) => {
        const confidence = (data.maxScore * 100).toFixed(0);
        const plural = data.count > 1 ? 's' : '';
        return `<div style="margin:2px 0;">‚Ä¢ ${data.count} ${name}${plural} (${confidence}%)</div>`;
      });
    
    objectsList.innerHTML = items.join('');
  }

  // Function to modify SDP for better codec compatibility
  function preferCompatibleCodecs(sdp) {
    console.log('üîß Optimizing SDP for codec compatibility...');
    
    // Prefer VP8 first (most compatible), then VP9, then H264
    const codecPreferences = ['VP8/90000', 'VP9/90000', 'H264/90000'];
    
    let lines = sdp.split('\n');
    let mLineIndex = -1;
    let codecLines = [];
    
    // Find video m-line and collect codec lines
    for (let i = 0; i < lines.length; i++) {
      if (lines[i].startsWith('m=video')) {
        mLineIndex = i;
        console.log('üì∫ Found video m-line:', lines[i]);
      } else if (lines[i].startsWith('a=rtpmap:') && lines[i].includes('video')) {
        codecLines.push({ index: i, line: lines[i] });
      }
    }
    
    if (mLineIndex !== -1 && codecLines.length > 0) {
      // Extract payload types from m-line
      const mLine = lines[mLineIndex];
      const parts = mLine.split(' ');
      const payloadTypes = parts.slice(3);
      
      console.log('üé¨ Available payload types:', payloadTypes);
      
      // Reorder payload types based on codec preference
      const reorderedTypes = [];
      
      codecPreferences.forEach(preferredCodec => {
        codecLines.forEach(codecLine => {
          if (codecLine.line.includes(preferredCodec)) {
            const payloadType = codecLine.line.split(':')[1].split(' ')[0];
            if (payloadTypes.includes(payloadType) && !reorderedTypes.includes(payloadType)) {
              reorderedTypes.push(payloadType);
              console.log(`‚úÖ Prioritizing ${preferredCodec} (payload ${payloadType})`);
            }
          }
        });
      });
      
      // Add remaining payload types
      payloadTypes.forEach(pt => {
        if (!reorderedTypes.includes(pt)) {
          reorderedTypes.push(pt);
        }
      });
      
      // Reconstruct m-line with reordered codecs
      lines[mLineIndex] = parts.slice(0, 3).join(' ') + ' ' + reorderedTypes.join(' ');
      console.log('üîÑ Reordered m-line:', lines[mLineIndex]);
    }
    
    const modifiedSDP = lines.join('\n');
    console.log('‚úÖ SDP optimization complete');
    return modifiedSDP;
  }

  // signaling
  socket.on('connect', () => {
    socket.emit('join', { room, role: 'receiver' });
    document.getElementById('status').innerHTML = 'üîó Connected to server, waiting for phone...';
    console.log('üîó Connected to server, joined room:', room);
  });

  socket.on('peer-joined', ({id, role}) => {
    // if a sender joined, just log it but wait for ready signal
    if (role === 'sender') {
      console.log('üì± Sender joined:', id);
      document.getElementById('status').innerHTML = 'üì± Phone connected! Waiting for camera...';
      senderId = id; // Store sender ID for when ready
      sendPendingIceCandidates(); // Send any ICE candidates that were generated early
    }
  });

  // Wait for sender to signal that tracks are added and create offer
  socket.on('sender-ready', (payload = {}) => {
    // Accept an optional senderId payload to handle race conditions where the
    // receiver didn't capture the earlier peer-joined event
    const id = payload.senderId || senderId;
    if (!id) {
      console.warn('sender-ready received but senderId unknown');
      return;
    }
    console.log('üéØ Sender ready with tracks, creating offer for', id);
    document.getElementById('status').innerHTML = 'üì± Phone ready! Creating connection...';
    
    // Reset connection attempts for new sender
    connectionAttempts = 0;
    
    // Store sender ID
    senderId = id;
    
    // Create offer since receiver should initiate
    createOffer(id);

    // Set a timeout for connection and retry once if needed
    setTimeout(() => {
      if (pc.connectionState === 'new' || pc.connectionState === 'connecting') {
        console.log('‚ö†Ô∏è Connection timeout - attempting retry...');
        document.getElementById('status').innerHTML = '‚ö†Ô∏è Connection timeout - retrying...';
        connectionAttempts++;
        if (connectionAttempts < MAX_CONNECTION_ATTEMPTS) {
          resetPeerConnection();
          createOffer(id);
        }
      }
    }, 10000); // 10 second timeout
  });

  socket.on('offer', async ({desc, from}) => {
    console.log('üì® Received offer from:', from);
    try {
      // Check if we need to reset the connection due to signaling state
      if (pc.signalingState !== 'stable' && pc.signalingState !== 'have-remote-offer') {
        console.log('‚ö†Ô∏è Invalid signaling state for offer:', pc.signalingState, '- resetting connection');
        resetPeerConnection();
      }
      
      await pc.setRemoteDescription(desc);
      console.log('‚úÖ Remote description set successfully');
      
      const answer = await pc.createAnswer();
      await pc.setLocalDescription(answer);
      socket.emit('answer', { to: from, desc: pc.localDescription });
      console.log('üì§ Sent answer to:', from);
    } catch (error) {
      console.error('‚ùå Error handling offer:', error);
      document.getElementById('status').innerHTML = '‚ùå Failed to process offer: ' + error.message;
      
      // Try to recover by resetting the connection
      setTimeout(() => {
        console.log('üîÑ Attempting to recover from offer error...');
        resetPeerConnection();
      }, 1000);
    }
  });
  
  socket.on('answer', async ({desc, from}) => {
    console.log('üì® Received answer from:', from);
    try {
      // Check signaling state before setting remote description
      if (pc.signalingState !== 'have-local-offer') {
        console.log('‚ö†Ô∏è Unexpected signaling state for answer:', pc.signalingState);
        if (pc.signalingState === 'stable') {
          console.log('‚ÑπÔ∏è Connection already stable, ignoring answer');
          return;
        }
      }
      
      // Set the senderId so ICE candidates can be sent to the correct peer
      if (from) {
        senderId = from;
        console.log('‚úÖ Set senderId to:', senderId);
        sendPendingIceCandidates(); // Send any ICE candidates that were generated early
      }
      
      await pc.setRemoteDescription(desc);
      console.log('‚úÖ Remote answer set successfully');
    } catch (error) {
      console.error('‚ùå Error handling answer:', error);
      document.getElementById('status').innerHTML = '‚ùå Failed to process answer: ' + error.message;
      
      // Try to recover by resetting the connection
      setTimeout(() => {
        console.log('üîÑ Attempting to recover from answer error...');
        resetPeerConnection();
        if (from) {
          senderId = from;
          createOffer(from);
        }
      }, 1000);
    }
  });
  
  socket.on('ice-candidate', async ({candidate}) => {
    console.log('üßä Received ICE candidate');
    try {
      // Check if we're in a valid state to add ICE candidates
      if (pc.remoteDescription) {
        await pc.addIceCandidate(candidate);
        console.log('‚úÖ ICE candidate added successfully');
      } else {
        console.log('‚è≥ Storing ICE candidate - no remote description yet');
        pendingIceCandidates.push(candidate);
      }
    } catch(error) {
      console.error('‚ùå ICE candidate error:', error);
      // Don't fail the entire connection for ICE candidate errors
    }
  });

  // Setup peer connection event handlers
  function setupPeerConnectionHandlers() {
    pc.onicecandidate = e => {
      console.log('üßä RECEIVER ICE candidate event:', e);
      console.log('üßä Current senderId:', senderId);
      console.log('üßä ICE gathering state:', pc.iceGatheringState);
      console.log('üßä ICE connection state:', pc.iceConnectionState);
      if (e.candidate) {
        console.log('üßä Sending ICE candidate:', e.candidate.type, e.candidate.candidate);
        console.log('üßä Full candidate:', e.candidate);
        if (senderId) {
          console.log('‚úÖ Sending to sender:', senderId);
          socket.emit('ice-candidate', { to: senderId, candidate: e.candidate, room: room });
        } else {
          console.log('‚è≥ Storing ICE candidate for later (senderId not yet set)');
          pendingIceCandidates.push(e.candidate);
        }
      } else {
        console.log('üßä ICE gathering complete');
      }
    };

    // Enhanced ICE gathering state monitoring
    pc.onicegatheringstatechange = () => {
      console.log('üì° RECEIVER ICE gathering state changed:', pc.iceGatheringState);
      if (pc.iceGatheringState === 'complete') {
        console.log('‚úÖ RECEIVER ICE gathering completed');
      } else if (pc.iceGatheringState === 'gathering') {
        console.log('‚è≥ RECEIVER ICE gathering in progress...');
      }
      
      // Force ICE restart if no candidates after 3 seconds
      if (pc.iceGatheringState === 'complete' && pendingIceCandidates.length === 0) {
        console.log('‚ö†Ô∏è RECEIVER No ICE candidates generated - forcing restart...');
        setTimeout(() => {
          if (pendingIceCandidates.length === 0) {
            console.log('üîÑ RECEIVER Restarting ICE gathering...');
            pc.restartIce();
          }
        }, 3000);
      }
    };

    pc.onconnectionstatechange = () => {
      console.log('üîó Connection state:', pc.connectionState);
      const status = document.getElementById('status');
      
      if (pc.connectionState === 'connected') {
        status.innerHTML = '‚úÖ WebRTC connected successfully!';
        status.className = 'success';
        connectionAttempts = 0; // Reset attempts on success
      } else if (pc.connectionState === 'failed') {
        status.innerHTML = '‚ùå WebRTC connection failed - retrying...';
        status.className = 'error';
        
        // Auto-retry with connection reset
        connectionAttempts++;
        if (connectionAttempts < MAX_CONNECTION_ATTEMPTS) {
          console.log(`üîÑ Connection attempt ${connectionAttempts}/${MAX_CONNECTION_ATTEMPTS} - resetting connection...`);
          setTimeout(() => {
            resetPeerConnection();
            if (senderId) {
              createOffer(senderId);
            }
          }, 2000);
        } else {
          console.log('‚ùå Max connection attempts reached');
          status.innerHTML = '‚ùå Connection failed after multiple attempts. Please refresh the page.';
        }
      } else if (pc.connectionState === 'connecting') {
        status.innerHTML = 'üîÑ Connecting to phone...';
        status.className = '';
      } else if (pc.connectionState === 'disconnected') {
        status.innerHTML = '‚ö†Ô∏è Phone disconnected - waiting for reconnection...';
        status.className = 'warning';
      } else if (pc.connectionState === 'new') {
        status.innerHTML = 'üÜï WebRTC connection initialized';
        status.className = '';
      }
    };

    // Monitor ICE connection state with more detail
    pc.oniceconnectionstatechange = () => {
      console.log('üîó RECEIVER ICE connection state changed:', pc.iceConnectionState);
      console.log('üîó ICE gathering state:', pc.iceGatheringState);
      const status = document.getElementById('status');
      if (pc.iceConnectionState === 'failed') {
        status.innerHTML = '‚ùå ICE connection failed - network connectivity issue';
      } else if (pc.iceConnectionState === 'connected') {
        console.log('‚úÖ ICE connected - media should flow now');
        status.innerHTML = 'üéâ Connected! Video should appear shortly...';
      } else if (pc.iceConnectionState === 'checking') {
        status.innerHTML = 'üîç Checking network connectivity...';
      } else if (pc.iceConnectionState === 'completed') {
        status.innerHTML = '‚úÖ Connection established successfully!';
      }
    };

    // Monitor signaling state
    pc.onsignalingstatechange = () => {
      console.log('üì° Signaling state:', pc.signalingState);
    };

    pc.ontrack = (evt) => {
      console.log('üì∫ Video track received:', evt);
      console.log('üì∫ Stream details:', evt.streams[0]);
      console.log('üì∫ Track details:', evt.track);
      
      const stream = evt.streams[0];
      video.srcObject = stream;
      
      // Force video to be visible and add debugging
      video.style.display = 'block';
      video.style.backgroundColor = 'black';
      console.log('üìπ Video element setup - srcObject assigned');
      
      // Enhanced video setup with multiple fallbacks
      const setupVideo = () => {
        console.log('üìπ Setting up video with metadata:', video.videoWidth, 'x', video.videoHeight);
        if (video.videoWidth > 0 && video.videoHeight > 0) {
          setupResponsiveCanvas();
          document.getElementById('status').innerHTML = 'üìπ Video stream active. Starting AI detection...';
          // Auto-start detection
          setTimeout(startAutoDetection, 1000);
          return true;
        }
        return false;
      };
      
      // Multiple event handlers to ensure video starts
      video.onloadedmetadata = () => {
        console.log('üìπ Video metadata loaded');
        setupVideo();
      };
      
      video.onloadeddata = () => {
        console.log('üìπ Video data loaded, attempting to play...');
        if (!setupVideo()) {
          // Retry after a short delay
          setTimeout(setupVideo, 100);
        }
        video.play().catch(e => {
          console.log('Video play attempt failed, will retry:', e.message);
          setTimeout(() => video.play().catch(console.warn), 1000);
        });
      };
      
      video.oncanplay = () => {
        console.log('üìπ Video can play');
        setupVideo();
      };
      
      // Force play with multiple attempts
      const attemptPlay = async (attempts = 3) => {
        for (let i = 0; i < attempts; i++) {
          try {
            await video.play();
            console.log('‚úÖ Video playing successfully');
            document.getElementById('status').innerHTML = '‚úÖ Video stream playing! Starting AI detection...';
            // Auto-start detection
            setTimeout(startAutoDetection, 1000);
            return;
          } catch (error) {
            console.log(`Video play attempt ${i + 1} failed:`, error.message);
            if (i === attempts - 1) {
              console.error('‚ùå All video play attempts failed:', error);
              document.getElementById('status').innerHTML = '‚ö†Ô∏è Video received but autoplay failed. Click video to play.';
              // Add click handler for manual play
              video.onclick = () => {
                video.play().then(() => {
                  console.log('‚úÖ Video playing after user click');
                  document.getElementById('status').innerHTML = '‚úÖ Video playing! Starting AI detection...';
                  video.onclick = null; // Remove click handler
                  setTimeout(startAutoDetection, 1000);
                }).catch(console.error);
              };
            } else {
              await new Promise(resolve => setTimeout(resolve, 500)); // Wait before retry
            }
          }
        }
      };
      
      // Start play attempts
      attemptPlay();

      // Track state monitoring
      evt.track.onended = () => {
        console.log('‚ö†Ô∏è Video track ended');
        document.getElementById('status').innerHTML = '‚ö†Ô∏è Video track ended - phone may have disconnected';
      };
      
      evt.track.onmute = () => {
        console.log('üîá Video track muted');
      };
      
      evt.track.onunmute = () => {
        console.log('üîä Video track unmuted');
      };
      
      // Add responsive canvas handlers
      video.addEventListener('resize', () => {
        console.log('üìè Video size changed');
        if (video.videoWidth > 0 && video.videoHeight > 0) {
          setupResponsiveCanvas();
        }
      });
      
      video.addEventListener('playing', () => {
        console.log('‚ñ∂Ô∏è Video playing event');
        setupResponsiveCanvas();
      });
    };
  }

  // Initialize peer connection handlers
  setupPeerConnectionHandlers();

  // Function to send pending ICE candidates when senderId becomes available
  function sendPendingIceCandidates() {
    if (senderId && pendingIceCandidates.length > 0) {
      console.log('üì§ Sending', pendingIceCandidates.length, 'pending ICE candidates to', senderId);
      pendingIceCandidates.forEach((candidate, index) => {
        console.log('üì§ Sending pending candidate', index + 1, ':', candidate.type);
        socket.emit('ice-candidate', { to: senderId, candidate: candidate, room: room });
      });
      pendingIceCandidates = []; // Clear the pending list
    }
  }
  
  // Handle window resize for responsive design
  window.addEventListener('resize', () => {
    setTimeout(() => {
      if (video && video.videoWidth > 0 && video.videoHeight > 0) {
        setupResponsiveCanvas();
      }
    }, 100);
  });

  async function createOffer(targetId) {
    try {
      console.log('üì§ Creating offer for:', targetId);
      
      // Check signaling state before creating offer
      if (pc.signalingState !== 'stable') {
        console.log('‚ö†Ô∏è Connection not in stable state:', pc.signalingState, '- resetting');
        resetPeerConnection();
      }
      
      // Ensure we have a video transceiver before creating offer
      const transceivers = pc.getTransceivers();
      const hasVideoTransceiver = transceivers.some(t => t.receiver && t.receiver.track && t.receiver.track.kind === 'video');
      
      if (!hasVideoTransceiver) {
        console.log('üì∫ Adding video transceiver in recvonly mode');
        pc.addTransceiver('video', { direction: 'recvonly' });
      } else {
        console.log('üì∫ Video transceiver already exists');
      }
      
      // Create offer with consistent options
      const offerOptions = {
        offerToReceiveVideo: true,
        offerToReceiveAudio: false,
        iceRestart: false // Prevent unnecessary ICE restart
      };
      
      const offer = await pc.createOffer(offerOptions);
      
      // Modify SDP to prefer compatible codecs and ensure consistent ordering
      offer.sdp = preferCompatibleCodecs(offer.sdp);
      
      await pc.setLocalDescription(offer);
      socket.emit('offer', { room, desc: pc.localDescription, to: targetId });
      console.log('‚úÖ Offer sent successfully with codec preferences');
      document.getElementById('status').innerHTML = 'üì° Connecting to phone...';
    } catch (error) {
      console.error('‚ùå Failed to create offer:', error);
      document.getElementById('status').innerHTML = '‚ùå Failed to create WebRTC offer: ' + error.message;
      
      // Try to recover by resetting connection
      connectionAttempts++;
      if (connectionAttempts < MAX_CONNECTION_ATTEMPTS) {
        console.log(`üîÑ Retrying offer creation (attempt ${connectionAttempts}/${MAX_CONNECTION_ATTEMPTS})`);
        setTimeout(() => {
          resetPeerConnection();
          createOffer(targetId);
        }, 2000);
      }
    }
  }

  // receive frame_meta messages from sender (frame alignment)
  socket.on('frame_meta', (meta) => {
    lastMeta = meta;
  });

  // receive server-mode detections (if server does inference)
  socket.on('detection', (det) => {
    // det expected to follow UX contract with normalized coords and timestamps
    overlayDetections(det.detections);
    detectionCount = det.detections.length;
    frameCount++;
    // compute E2E for metrics: overlay_display_ts - capture_ts
    const e2e = Date.now() - (det.capture_ts || Date.now());
    e2eLatencies.push(e2e);
    updateStats();
  });

  // CLIENT (ENHANCED) MODEL LOOP - supports ONNX, TensorFlow Lite, and COCO-SSD
  let running = false;
  
  async function startAutoDetection() {
    if (running) return;
    
    document.getElementById('status').innerHTML = 'üß† Loading best available AI model...';
    
    try {
      const success = await modelManager.loadBestAvailableModel();
      if (!success) {
        throw new Error('Failed to load any AI model');
      }
      
      const modelInfo = modelManager.getModelInfo();
      document.getElementById('status').innerHTML = `üéØ ${modelInfo} loaded! Running real-time detection...`;
      console.log(`üöÄ Using model: ${modelInfo}`);
      
      // Update the model info display
      document.getElementById('modelInfo').textContent = modelInfo;
      
      running = true;
      runDetectLoop();
    } catch (error) {
      console.error('Model loading error:', error);
      document.getElementById('status').innerHTML = '‚ùå Failed to load AI model: ' + error.message;
    }
  }

  async function runDetectLoop(){
    const off = document.createElement('canvas');
    const offCtx = off.getContext('2d');
    const targetFPS = 5; // Reduce FPS for better performance
    const interval = Math.round(1000/targetFPS);
    
    console.log('üéØ Starting detection loop...');
    
    while(running){
      try {
        // Check if video is playing and has dimensions
        if (video.videoWidth === 0 || video.videoHeight === 0 || video.paused) { 
          await new Promise(r=>setTimeout(r,100)); 
          continue; 
        }
        
        // Set canvas size to match video
        off.width = video.videoWidth;
        off.height = video.videoHeight;
        offCtx.drawImage(video, 0, 0, off.width, off.height);
        
        console.log('üîç Running detection on frame:', off.width, 'x', off.height);
        
        const startInference = Date.now();
        // Use the enhanced model manager for detection
        const predictions = await modelManager.detect(off);
        const inference_ts = Date.now();
        
        console.log('üìä Detected objects:', predictions.length, predictions);
        
        // Predictions are already in normalized format from model manager
        const detections = predictions.filter(p => p.score > 0.3); // Filter low confidence
        
        console.log('üìä High confidence detections:', detections.length);

        // attach frame alignment using lastMeta
        const meta = lastMeta || { frame_id: 'na', capture_ts: Date.now() };
        const recv_ts = Date.now();
        const payload = {
          frame_id: meta.frame_id,
          capture_ts: meta.capture_ts,
          recv_ts,
          inference_ts,
          detections
        };

        // overlay detections
        overlayDetections(detections);

        // Store latest detections for the objects list
        latestDetections = detections;

        // update detection count and stats
        detectionCount = detections.length;
        frameCount++;

        // record metrics: overlay_display_ts - capture_ts
        const overlay_ts = Date.now();
        e2eLatencies.push(overlay_ts - (meta.capture_ts || overlay_ts));
        // store for export
        detectionsHistory.push(payload);

        // ========== METRICS INTEGRATION (EXTRA CODE) ==========
        // Collect metrics for benchmark without disturbing existing code
        if (benchmarkActive) {
          // Record latency
          const latency = overlay_ts - (meta.capture_ts || overlay_ts);
          metricsData.latencies.push(latency);
          
          // Record frame processed
          metricsData.totalFrames++;
          
          // Record objects detected
          metricsData.totalObjects += detections.length;
          
          // Calculate current FPS for this detection cycle
          const currentFPS = 1000 / interval; // Based on detection interval
          metricsData.fps.push(currentFPS);
        }
        // ========== END METRICS INTEGRATION ==========

        // update stats display periodically
        if (frameCount % 5 === 0) {
          updateStats();
        }

        // Show detected objects in status
        if (detections.length > 0) {
          const objectNames = detections.map(d => `${d.label} (${(d.score*100).toFixed(0)}%)`).join(', ');
          document.getElementById('status').innerHTML = `üéØ Detecting: ${objectNames}`;
        } else {
          document.getElementById('status').innerHTML = 'üîç Scanning for objects...';
        }

      } catch (error) {
        console.error('Detection error:', error);
        document.getElementById('status').innerHTML = '‚ùå Detection error: ' + error.message;
      }

      await new Promise(r => setTimeout(r, interval));
    }
  }

  // Setup responsive canvas that matches video display size
  function setupResponsiveCanvas() {
    if (!video || !overlay || video.videoWidth === 0 || video.videoHeight === 0) {
      return;
    }
    
    // Set canvas internal size to match video resolution
    overlay.width = video.videoWidth;
    overlay.height = video.videoHeight;
    
    // Calculate video display dimensions
    const videoRect = video.getBoundingClientRect();
    const videoDisplayWidth = videoRect.width;
    const videoDisplayHeight = videoRect.height;
    
    // Set canvas display size to match video display size
    overlay.style.width = `${videoDisplayWidth}px`;
    overlay.style.height = `${videoDisplayHeight}px`;
    
    console.log('üé® Canvas setup:', {
      videoResolution: `${video.videoWidth}x${video.videoHeight}`,
      videoDisplay: `${videoDisplayWidth}x${videoDisplayHeight}`,
      canvasSize: `${overlay.width}x${overlay.height}`
    });
  }

  function overlayDetections(detections){
    // Ensure canvas matches video dimensions and setup responsive display
    overlay.width = video.videoWidth;
    overlay.height = video.videoHeight;
    setupResponsiveCanvas();
    ctx.clearRect(0,0,overlay.width, overlay.height);
    
    console.log('üé® Drawing', detections.length, 'detections on', overlay.width, 'x', overlay.height, 'canvas');
    
    if (detections.length === 0) return;
    
    ctx.lineWidth = 3;
    ctx.font = 'bold 16px Arial';
    ctx.shadowColor = 'rgba(0,0,0,0.5)';
    ctx.shadowBlur = 2;
    
    detections.forEach((d, index) => {
      const x = d.xmin * overlay.width;
      const y = d.ymin * overlay.height;
      const w = (d.xmax - d.xmin) * overlay.width;
      const h = (d.ymax - d.ymin) * overlay.height;
      
      // Use bright, contrasting colors
      const colors = ['#FF3B30', '#FF9500', '#FFCC02', '#34C759', '#007AFF', '#5856D6', '#AF52DE', '#FF2D92'];
      const color = colors[index % colors.length];
      
      // Draw bounding box
      ctx.strokeStyle = color;
      ctx.strokeRect(x, y, w, h);
      
      // Draw label background
      const txt = `${d.label} ${(d.score*100).toFixed(0)}%`;
      const textMetrics = ctx.measureText(txt);
      const textWidth = textMetrics.width + 12;
      const textHeight = 28;
      
      ctx.fillStyle = color;
      ctx.fillRect(x, y - textHeight, textWidth, textHeight);
      
      // Draw label text
      ctx.fillStyle = 'white';
      ctx.fillText(txt, x + 6, y - 8);
      
      console.log('‚úÖ Drew detection:', txt, 'at', x.toFixed(0), y.toFixed(0));
    });
    
    // Reset shadow
    ctx.shadowColor = 'transparent';
    ctx.shadowBlur = 0;
  }

  // expose metrics functions for bench script
  window.metricsReady = true;
  window.getMetrics = () => {
    const e2eSorted = [...e2eLatencies].sort((a,b)=>a-b);
    function pct(arr,p){ if(arr.length===0) return null; const i = Math.floor((p/100)*(arr.length-1)); return arr[i];}
    const median = pct(e2eSorted,50);
    const p95 = pct(e2eSorted,95);
    const fps = detectionsHistory.length / ( (e2eLatencies.length>0 ? (e2eLatencies.length* (1000/10)) : 30) / 1000 );
    // fps estimation is approximate; bench script should run in controlled loop to give accurate FPS
    return {
      e2e_median_ms: median,
      e2e_p95_ms: p95,
      processed_fps: detectionsHistory.length / Math.max(1, ((Date.now() - (detectionsHistory[0]?.recv_ts || Date.now()))/1000)),
      samples: detectionsHistory.length
    };
  };

  document.getElementById('exportMetrics').onclick = () => {
    const m = window.getMetrics();
    const data = JSON.stringify(m, null, 2);
    const a = document.createElement('a');
    a.href = URL.createObjectURL(new Blob([data], {type:'application/json'}));
    a.download = 'metrics.json';
    a.click();
  };

  // Initialize stats update interval
  setInterval(updateStats, 1000);

    // Test function to verify video display
  window.testVideoDisplay = function() {
    console.log('üîß Testing video display...');
    console.log('Video element:', video);
    console.log('Video srcObject:', video.srcObject);
    console.log('Video readyState:', video.readyState);
    console.log('Video dimensions:', video.videoWidth, 'x', video.videoHeight);
    console.log('Video paused:', video.paused);
    console.log('Video muted:', video.muted);
    console.log('Video style display:', video.style.display);
    console.log('Video current time:', video.currentTime);
    console.log('Video duration:', video.duration);
    console.log('WebRTC connection state:', pc.connectionState);
    console.log('WebRTC ice connection state:', pc.iceConnectionState);
    console.log('WebRTC signaling state:', pc.signalingState);
    
    if (video.srcObject) {
      const tracks = video.srcObject.getVideoTracks();
      console.log('Video tracks:', tracks.length);
      tracks.forEach((track, i) => {
        console.log(`Track ${i}:`, track.label, 'enabled:', track.enabled, 'state:', track.readyState);
      });
      
      // Force video to play and show
      video.style.display = 'block';
      video.style.visibility = 'visible';
      video.style.opacity = '1';
      video.play().then(() => {
        console.log('‚úÖ Video play successful');
        document.getElementById('status').innerHTML = '‚úÖ Video test successful!';
      }).catch(e => {
        console.error('‚ùå Video play failed:', e);
        document.getElementById('status').innerHTML = '‚ùå Video play failed: ' + e.message;
      });
    } else {
      console.log('‚ùå No video stream available');
      document.getElementById('status').innerHTML = '‚ùå No video stream - check phone connection';
    }
    
    // Test WebRTC stats
    if (pc.getStats) {
      pc.getStats().then(stats => {
        console.log('üìä WebRTC Stats:');
        stats.forEach(report => {
          if (report.type === 'inbound-rtp' && report.mediaType === 'video') {
            console.log('üìπ Video inbound stats:', report);
          }
          if (report.type === 'track' && report.kind === 'video') {
            console.log('üìπ Video track stats:', report);
          }
        });
      }).catch(console.error);
    }
  };

  // Add a debug button
  window.debugWebRTC = function() {
    console.log('üêõ WebRTC Debug Info:');
    console.log('Connection State:', pc.connectionState);
    console.log('ICE Connection State:', pc.iceConnectionState);
    console.log('ICE Gathering State:', pc.iceGatheringState);
    console.log('Signaling State:', pc.signalingState);
    console.log('Sender ID:', senderId);
    console.log('Pending ICE Candidates:', pendingIceCandidates.length);
    
    const transceivers = pc.getTransceivers();
    console.log('Transceivers:', transceivers.length);
    transceivers.forEach((t, i) => {
      console.log(`Transceiver ${i}:`, {
        direction: t.direction,
        receiver: t.receiver?.track?.readyState,
        sender: t.sender?.track?.readyState
      });
    });
  };

  // ========== METRICS COLLECTION FUNCTIONS ==========

  function startBenchmark() {
    if (benchmarkActive) return;
    
    benchmarkActive = true;
    benchmarkStartTime = Date.now();
    metricsData.startTime = new Date().toISOString();
    metricsData.latencies = [];
    metricsData.fps = [];
    metricsData.uplink = [];
    metricsData.downlink = [];
    metricsData.totalFrames = 0;
    metricsData.totalObjects = 0;
    
    // Update UI
    document.getElementById('startBenchBtn').disabled = true;
    document.getElementById('benchStatus').style.display = 'block';
    document.getElementById('benchStatus').innerHTML = 'Benchmark running... (30s remaining)';
    
    // Start collecting metrics
    statsInterval = setInterval(collectMetrics, 1000);
    
    // Stop after 30 seconds
    setTimeout(() => {
      stopBenchmark();
    }, 30000);
    
    console.log('üìä Started 30-second benchmark');
  }

  function stopBenchmark() {
    if (!benchmarkActive) return;
    
    benchmarkActive = false;
    metricsData.endTime = new Date().toISOString();
    
    if (statsInterval) {
      clearInterval(statsInterval);
      statsInterval = null;
    }
    
    // Calculate final metrics
    const finalMetrics = calculateFinalMetrics();
    
    // Update UI
    document.getElementById('startBenchBtn').disabled = false;
    document.getElementById('downloadMetricsBtn').disabled = false;
    document.getElementById('benchStatus').style.display = 'none';
    
    console.log('üìä Benchmark completed:', finalMetrics);
  }

  function collectMetrics() {
    if (!benchmarkActive) return;
    
    // Update benchmark status only - live metrics handled separately
    const timeRemaining = Math.max(0, 30 - Math.floor((Date.now() - benchmarkStartTime) / 1000));
    document.getElementById('benchStatus').innerHTML = `Benchmark running... (${timeRemaining}s remaining)`;
  }

  function calculateFinalMetrics() {
    const sortedLatencies = [...metricsData.latencies].sort((a, b) => a - b);
    const medianLatency = sortedLatencies.length > 0 ? 
      sortedLatencies[Math.floor(sortedLatencies.length * 0.5)] : 0;
    const p95Latency = sortedLatencies.length > 0 ? 
      sortedLatencies[Math.floor(sortedLatencies.length * 0.95)] : 0;
    
    const avgFPS = metricsData.fps.length > 0 ? 
      metricsData.fps.reduce((a, b) => a + b, 0) / metricsData.fps.length : 0;
    
    const avgUplink = metricsData.uplink.length > 0 ? 
      metricsData.uplink.reduce((a, b) => a + b, 0) / metricsData.uplink.length : 0;
    
    const avgDownlink = metricsData.downlink.length > 0 ? 
      metricsData.downlink.reduce((a, b) => a + b, 0) / metricsData.downlink.length : 0;
    
    return {
      benchmark_duration_seconds: 30,
      end_to_end_latency: {
        median_ms: Math.round(medianLatency),
        p95_ms: Math.round(p95Latency),
        samples: sortedLatencies.length
      },
      processed_fps: {
        average: Math.round(avgFPS),
        samples: metricsData.fps.length
      },
      network_throughput: {
        uplink_kbps: Math.round(avgUplink),
        downlink_kbps: Math.round(avgDownlink)
      },
      detection_stats: {
        total_frames_processed: metricsData.totalFrames,
        total_objects_detected: metricsData.totalObjects
      },
      timestamp: {
        start: metricsData.startTime,
        end: metricsData.endTime
      }
    };
  }

  function downloadMetrics() {
    const finalMetrics = calculateFinalMetrics();
    
    const blob = new Blob([JSON.stringify(finalMetrics, null, 2)], {
      type: 'application/json'
    });
    
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `metrics_${new Date().toISOString().replace(/[:.]/g, '-')}.json`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
    
    console.log('üì• Downloaded metrics.json');
  }

  // Update live metrics display
  function updateLiveMetrics() {
    console.log('üìä Updating live metrics - Objects:', latestDetections.length);
    
    // Update object count (only visible metric)
    document.getElementById('objectCount').textContent = latestDetections.length;
    
    // ========== COLLECT DATA FOR BENCHMARK (BACKGROUND) ==========
    // Continue collecting all metrics for benchmark even if not displayed
    if (benchmarkActive) {
      // Collect latency data
      if (e2eLatencies.length > 0) {
        const latestLatency = e2eLatencies[e2eLatencies.length - 1];
        metricsData.latencies.push(latestLatency);
      }
      
      // Collect WebRTC stats for FPS and bandwidth
      if (pc && (pc.connectionState === 'connected' || pc.connectionState === 'connecting')) {
        pc.getStats().then(stats => {
          let bytesReceived = 0;
          let bytesSent = 0;
          let currentFPS = 0;
          
          stats.forEach(report => {
            if (report.type === 'inbound-rtp' && report.mediaType === 'video') {
              bytesReceived = report.bytesReceived || 0;
              currentFPS = report.framesPerSecond || 0;
            }
            if (report.type === 'outbound-rtp' && report.mediaType === 'video') {
              bytesSent = report.bytesSent || 0;
            }
            if (report.type === 'candidate-pair' && report.state === 'succeeded') {
              if (report.bytesReceived) bytesReceived = Math.max(bytesReceived, report.bytesReceived);
              if (report.bytesSent) bytesSent = Math.max(bytesSent, report.bytesSent);
            }
          });
          
          const now = Date.now();
          const timeDiff = (now - lastStatsTime) / 1000;
          
          if (timeDiff > 0.5) {
            // Calculate throughput for benchmark data
            if (bytesReceived > lastBytesReceived) {
              const downlinkKbps = ((bytesReceived - lastBytesReceived) * 8) / (timeDiff * 1000);
              if (downlinkKbps > 0) {
                metricsData.downlink.push(Math.round(downlinkKbps));
              }
            }
            
            if (bytesSent > lastBytesSent) {
              const uplinkKbps = ((bytesSent - lastBytesSent) * 8) / (timeDiff * 1000);
              if (uplinkKbps > 0) {
                metricsData.uplink.push(Math.round(uplinkKbps));
              }
            }
            
            lastBytesReceived = bytesReceived;
            lastBytesSent = bytesSent;
            lastStatsTime = now;
          }
          
          // Collect FPS data
          if (currentFPS > 0) {
            metricsData.fps.push(currentFPS);
          }
          
        }).catch(error => {
          console.log('‚ùå Stats collection error:', error.message);
        });
      }
      
      // Update benchmark counters
      metricsData.totalFrames++;
      metricsData.totalObjects += latestDetections.length;
    }
    // ========== END BENCHMARK DATA COLLECTION ==========
  }

  // Start live metrics updates
  setInterval(updateLiveMetrics, 1000);

  // ========== INITIALIZE LIVE METRICS DISPLAY (SIMPLIFIED) ==========
  // Initialize metrics display with default values
  function initializeMetricsDisplay() {
    document.getElementById('objectCount').textContent = '0';
    console.log('üìä Metrics display initialized');
  }
  
  // Call initialization when page loads
  initializeMetricsDisplay();
  
  // Show connection status in console for debugging
  function logConnectionStatus() {
    if (pc) {
      console.log('üîó WebRTC Connection State:', pc.connectionState);
      console.log('üßä ICE Connection State:', pc.iceConnectionState);
    } else {
      console.log('‚ùå No WebRTC peer connection');
    }
  }
  
  // Monitor connection status less frequently
  setInterval(logConnectionStatus, 10000);

  // Initialize metrics on page load
  initializeMetricsDisplay();

  // Debug: Log metrics updates
  setInterval(() => {
    console.log('üìä Metrics Debug:', {
      connectionState: pc.connectionState,
      e2eLatenciesCount: e2eLatencies.length,
      frameCount: frameCount,
      latestDetections: latestDetections.length,
      lastStatsTime: new Date(lastStatsTime).toISOString()
    });
  }, 5000);
  // ========== END INITIALIZE LIVE METRICS DISPLAY ==========

  </script>
</body>
</html>